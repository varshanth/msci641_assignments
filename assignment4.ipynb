{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from lib.amazon_reviews_loader import AmazonReviewsDS\n",
    "from lib.amazon_reviews_cfg import DS_CFG_NO_SW, DS_CFG_SW\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from copy import copy\n",
    " \n",
    "_POS_REV_FILE = 'dataset/pos.txt'\n",
    "_NEG_REV_FILE = 'dataset/neg.txt'\n",
    "_WORD2VEC_EMBEDDING = 'dataset/word2vec_embeddings.kv'\n",
    "_MAX_VOCAB_SIZE = 20000\n",
    "_EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_rev_sw = AmazonReviewsDS(_POS_REV_FILE, _NEG_REV_FILE, DS_CFG_SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max_len_sentence = int(np.percentile([len(rev) for rev in amazon_rev_sw.data], 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting Tokenizer on Dataset')\n",
    "tokenizer = Tokenizer(num_words = _MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts([' '.join(rev[:_max_len_sentence]) for rev in amazon_rev_sw.data])\n",
    "X = tokenizer.texts_to_sequences([' '.join(rev[:_max_len_sentence]) for rev in amazon_rev_sw.data])\n",
    "X = pad_sequences(X, maxlen=_max_len_sentence, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Splitting Dataset into Train, Val & Test')\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, amazon_rev_sw.labels, random_state=10, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, random_state=10, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating local embeddings matrix from Word2Vec Embeddings')\n",
    "word2vec_embeddings = models.KeyedVectors.load(_WORD2VEC_EMBEDDING, mmap='r')\n",
    "num_unique_tokens = len(tokenizer.word_index)+1  # +1 is because 0th index corresponds to pad char\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(num_unique_tokens, _EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = word2vec_embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del word2vec_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating NN Model')\n",
    "_NUM_HIDDEN_UNITS = 1024\n",
    "\n",
    "def create_nn_model(activation_fn, dropout_rate, reg_param):\n",
    "    print(f'Activation: {activation_fn}')\n",
    "    print(f'DropoutRate: {dropout_rate}')\n",
    "    print(f'RegParam: {reg_param}')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = num_unique_tokens,\n",
    "                        output_dim = _EMBEDDING_DIM,\n",
    "                        weights = [embeddings_matrix],\n",
    "                        trainable=False,\n",
    "                        name='word_embedding_layer', \n",
    "                        input_length = _max_len_sentence))\n",
    "    # Output will be [batch size, input_length, output_dim]\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(_NUM_HIDDEN_UNITS,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer = l2(reg_param),\n",
    "                    name='hidden_layer'))\n",
    "    model.add(Dropout(rate=0.1, name='dropout_layer'))\n",
    "    model.add(Dense(1, activation = 'sigmoid', name = 'output_layer'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_N_EPOCHS = 10\n",
    "_BATCH_SIZE = 32\n",
    "\n",
    "from collections import OrderedDict\n",
    "hyperparameters = OrderedDict()\n",
    "hyperparameters['activation_fn'] = ['sigmoid', 'tanh', 'relu']\n",
    "hyperparameters['dropout_rate'] = [0, 0.1, 0.3, 0.5]\n",
    "hyperparameters['reg_param'] = [0, 0.1, 0.01, 0.001]\n",
    "\n",
    "selected_hyperparam = {values[0] for hyp, values in hyperparameters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hyperparam Sequential Search')\n",
    "for hyperparam, values in hyperparameters.items():\n",
    "    best_value_valacc_seen_so_far = [None, -1]\n",
    "    for value in values:\n",
    "        print(f'Hyperparam: {hyperparam}, Value: {value}')\n",
    "        es = EarlyStopping(monitor='val_acc', mode = 'max', patience = 2, verbose=1)\n",
    "        passed_hyperparam = copy(selected_hyperparam)\n",
    "        passed_hyperparam[hyperparam] = value\n",
    "        model = create_nn_model(**passed_hyperparam)\n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size = _BATCH_SIZE,\n",
    "                  epochs = _N_EPOCHS,\n",
    "                  validation_data = (X_val, y_val),\n",
    "                  callbacks=[es])\n",
    "        _, val_acc = model.evaluate(X_val, y_val)\n",
    "        if val_acc > best_value_valacc_seen_so_far[1]:\n",
    "            best_value_valacc_seen_so_far[0] = value\n",
    "            best_value_valacc_seen_so_far[1] = val_acc\n",
    "    selected_hyperparam[hyperparam] = best_value_valacc_seen_so_far[0]\n",
    "\n",
    "print(f'Selected Hyperparameters: {selected_hyperparam}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retraining Best Model With Selected Hyperparam')\n",
    "model = create_nn_model(**selected_hyperparam)\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size = _BATCH_SIZE,\n",
    "          epochs = _N_EPOCHS,\n",
    "          validation_data = (X_val, y_val),\n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Best Model')\n",
    "loss_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {loss_acc[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
