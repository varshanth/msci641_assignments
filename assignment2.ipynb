{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/varsrao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Retrieving Amazon Reviews Dataset with No Stopwords\n",
      "----- Dataset Synthesis Start -----\n",
      "Loading Positive Reviews from dataset/pos.txt\n",
      "Loading Negative Reviews from dataset/neg.txt\n",
      "Generating data and labels\n",
      "Tokenizing the data\n",
      "Removing stop words\n",
      "Shuffling the data\n",
      "----- Dataset Synthesis Complete -----\n",
      "Retrieving Amazon Reviews Dataset with Stopwords\n",
      "----- Dataset Synthesis Start -----\n",
      "Loading Positive Reviews from dataset/pos.txt\n",
      "Loading Negative Reviews from dataset/neg.txt\n",
      "Generating data and labels\n",
      "Tokenizing the data\n",
      "Shuffling the data\n",
      "----- Dataset Synthesis Complete -----\n",
      "Splitting the datasets into train, validation and test sets\n"
     ]
    }
   ],
   "source": [
    "from lib.amazon_reviews_loader import AmazonReviewsDS\n",
    "from lib.amazon_reviews_cfg import DS_CFG_NO_SW, DS_CFG_SW\n",
    "\n",
    "_POS_REV_FILE = 'dataset/pos.txt'\n",
    "_NEG_REV_FILE = 'dataset/neg.txt'\n",
    "\n",
    "def review_list_of_tokens_to_corpus(review_list):\n",
    "    return [\" \".join(review) for review in review_list]\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    print('Retrieving Amazon Reviews Dataset with No Stopwords')\n",
    "    amazon_rev_no_sw = AmazonReviewsDS(_POS_REV_FILE, _NEG_REV_FILE, DS_CFG_NO_SW)\n",
    "    print('Retrieving Amazon Reviews Dataset with Stopwords')\n",
    "    amazon_rev_sw = AmazonReviewsDS(_POS_REV_FILE, _NEG_REV_FILE, DS_CFG_SW)\n",
    "\n",
    "    print('Splitting the datasets into train, validation and test sets')\n",
    "    amazon_rev_no_sw_splits = {\n",
    "            'train' : (review_list_of_tokens_to_corpus(amazon_rev_no_sw.get_train_data()[0]),\n",
    "                amazon_rev_no_sw.get_train_data()[1]),\n",
    "            'val': (review_list_of_tokens_to_corpus(amazon_rev_no_sw.get_val_data()[0]),\n",
    "                amazon_rev_no_sw.get_val_data()[1]),\n",
    "            'test' : (review_list_of_tokens_to_corpus(amazon_rev_no_sw.get_test_data()[0]),\n",
    "                amazon_rev_no_sw.get_test_data()[1])\n",
    "            }\n",
    "\n",
    "    amazon_rev_sw_splits = {\n",
    "            'train' : (review_list_of_tokens_to_corpus(amazon_rev_sw.get_train_data()[0]),\n",
    "                amazon_rev_sw.get_train_data()[1]),\n",
    "            'val': (review_list_of_tokens_to_corpus(amazon_rev_sw.get_val_data()[0]),\n",
    "                amazon_rev_sw.get_val_data()[1]),\n",
    "            'test' : (review_list_of_tokens_to_corpus(amazon_rev_sw.get_test_data()[0]),\n",
    "                amazon_rev_sw.get_test_data()[1])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Count Vectorizers\n"
     ]
    }
   ],
   "source": [
    "print('Creating Count Vectorizers')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "unigram_count_vec = CountVectorizer()\n",
    "bigram_count_vec = CountVectorizer(ngram_range=(2,2))\n",
    "unigram_bigram_count_vec = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fitting Count Vectorizers to the Entire DataSet: With Stopwords\n",
      "Unigram Count Shape: (800000, 80956)\n",
      "Bigram Count Shape: (800000, 1651114)\n",
      "Unigram+Bigram Count Shape: (800000, 1732070)\n",
      "--------------------------------------------------\n",
      "Fitting Count Vectorizers to the Entire DataSet: Without Stopwords\n",
      "Unigram Count Shape: (800000, 80813)\n",
      "Bigram Count Shape: (800000, 2206262)\n",
      "Unigram+Bigram Count Shape: (800000, 2287075)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf we use 3 seperate count vectorizers for train, val & test set, since the vocab\\nmay be different in both sets, the feature (token count) representations will\\nbe different (because either the vocab or the vocab size would be different)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('--------------------------------------------------')\n",
    "print('Fitting Count Vectorizers to the Entire DataSet: With Stopwords')\n",
    "amazon_rev_sw_uni_counts = unigram_count_vec.fit_transform(\n",
    "    amazon_rev_sw_splits['train'][0] +\n",
    "    amazon_rev_sw_splits['val'][0] +\n",
    "    amazon_rev_sw_splits['test'][0])\n",
    "amazon_rev_sw_big_counts = bigram_count_vec.fit_transform(\n",
    "    amazon_rev_sw_splits['train'][0] +\n",
    "    amazon_rev_sw_splits['val'][0] +\n",
    "    amazon_rev_sw_splits['test'][0])\n",
    "amazon_rev_sw_uni_big_counts = unigram_bigram_count_vec.fit_transform(\n",
    "    amazon_rev_sw_splits['train'][0] +\n",
    "    amazon_rev_sw_splits['val'][0] +\n",
    "    amazon_rev_sw_splits['test'][0])\n",
    "print(f'Unigram Count Shape: {amazon_rev_sw_uni_counts.shape}')\n",
    "print(f'Bigram Count Shape: {amazon_rev_sw_big_counts.shape}')\n",
    "print(f'Unigram+Bigram Count Shape: {amazon_rev_sw_uni_big_counts.shape}')\n",
    "print('--------------------------------------------------')\n",
    "print('Fitting Count Vectorizers to the Entire DataSet: Without Stopwords')\n",
    "amazon_rev_no_sw_uni_counts = unigram_count_vec.fit_transform(\n",
    "    amazon_rev_no_sw_splits['train'][0] + \n",
    "    amazon_rev_no_sw_splits['val'][0] +\n",
    "    amazon_rev_no_sw_splits['test'][0])\n",
    "amazon_rev_no_sw_big_counts = bigram_count_vec.fit_transform(\n",
    "    amazon_rev_no_sw_splits['train'][0] +\n",
    "    amazon_rev_no_sw_splits['val'][0] +\n",
    "    amazon_rev_no_sw_splits['test'][0])\n",
    "amazon_rev_no_sw_uni_big_counts = unigram_bigram_count_vec.fit_transform(\n",
    "    amazon_rev_no_sw_splits['train'][0] +\n",
    "    amazon_rev_no_sw_splits['val'][0] +\n",
    "    amazon_rev_no_sw_splits['test'][0])\n",
    "print(f'Unigram Count Shape: {amazon_rev_no_sw_uni_counts.shape}')\n",
    "print(f'Bigram Count Shape: {amazon_rev_no_sw_big_counts.shape}')\n",
    "print(f'Unigram+Bigram Count Shape: {amazon_rev_no_sw_uni_big_counts.shape}')\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "'''\n",
    "If we use 3 seperate count vectorizers for train, val & test set, since the vocab\n",
    "may be different in both sets, the feature (token count) representations will\n",
    "be different (because either the vocab or the vocab size would be different)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sw_train = len(amazon_rev_sw_splits['train'][1])\n",
    "len_sw_val = len(amazon_rev_sw_splits['val'][1])\n",
    "len_no_sw_train = len(amazon_rev_no_sw_splits['train'][1])\n",
    "len_no_sw_val = len(amazon_rev_no_sw_splits['val'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & Validating Classifiers\n",
      "Alpha = 0.1\n",
      "Alpha = 0.2\n",
      "Alpha = 0.30000000000000004\n",
      "Alpha = 0.4\n",
      "Alpha = 0.5\n",
      "Alpha = 0.6\n",
      "Alpha = 0.7000000000000001\n",
      "Alpha = 0.8\n",
      "Alpha = 0.9\n",
      "Alpha = 1.0\n",
      "Best Alpha for Without Stopwords: 0.7000000000000001\n",
      "Best Alpha for With Stopwords: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "def calculate_avg_accuracy(pred, gt):\n",
    "    return np.mean(pred==gt)\n",
    "\n",
    "'''\n",
    "Train only using the train count vectors\n",
    "Validate only using the val count vectors\n",
    "'''\n",
    "\n",
    "alpha_no_sw = alpha_sw = 0\n",
    "best_val_seen_so_far_no_sw = best_val_seen_so_far_sw = 0\n",
    "\n",
    "print('Training & Validating Classifiers')\n",
    "for alpha in np.arange(0.1, 1.1, 0.1):\n",
    "    print(f'Alpha = {alpha}')\n",
    "    # Training Without SW\n",
    "    unigram_no_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_no_sw_uni_counts[:len_no_sw_train],\n",
    "        amazon_rev_no_sw_splits['train'][1])\n",
    "    bigram_no_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_no_sw_big_counts[:len_no_sw_train],\n",
    "        amazon_rev_no_sw_splits['train'][1])\n",
    "    unigram_bigram_no_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_no_sw_uni_big_counts[:len_no_sw_train],\n",
    "        amazon_rev_no_sw_splits['train'][1])\n",
    "    \n",
    "    # Training With SW\n",
    "    unigram_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_sw_uni_counts[:len_sw_train],\n",
    "        amazon_rev_sw_splits['train'][1])\n",
    "    bigram_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_sw_big_counts[:len_sw_train],\n",
    "        amazon_rev_sw_splits['train'][1])\n",
    "    unigram_bigram_sw_classifier = MultinomialNB(alpha=alpha).fit(\n",
    "        amazon_rev_sw_uni_big_counts[:len_sw_train],\n",
    "        amazon_rev_sw_splits['train'][1])\n",
    "\n",
    "    # Validation Without SW\n",
    "    pred_no_sw_uni = unigram_no_sw_classifier.predict(\n",
    "        amazon_rev_no_sw_uni_counts[\n",
    "            len_no_sw_train:len_no_sw_train+len_no_sw_val])\n",
    "    pred_no_sw_big = bigram_no_sw_classifier.predict(\n",
    "        amazon_rev_no_sw_big_counts[\n",
    "            len_no_sw_train:len_no_sw_train+len_no_sw_val])\n",
    "    pred_no_sw_uni_big = unigram_bigram_no_sw_classifier.predict(\n",
    "        amazon_rev_no_sw_uni_big_counts[\n",
    "            len_no_sw_train:len_no_sw_train+len_no_sw_val])\n",
    "    \n",
    "    # Validation With SW\n",
    "    pred_sw_uni = unigram_sw_classifier.predict(\n",
    "        amazon_rev_sw_uni_counts[\n",
    "            len_sw_train:len_sw_train+len_sw_val])\n",
    "    pred_sw_big = bigram_sw_classifier.predict(\n",
    "        amazon_rev_sw_big_counts[\n",
    "            len_sw_train:len_sw_train+len_sw_val])\n",
    "    pred_sw_uni_big = unigram_bigram_sw_classifier.predict(\n",
    "        amazon_rev_sw_uni_big_counts[\n",
    "            len_sw_train:len_sw_train+len_sw_val])\n",
    "    \n",
    "    # Validation Accuracy Without SW\n",
    "    uni_no_sw_acc = calculate_avg_accuracy(\n",
    "        pred_no_sw_uni, amazon_rev_sw_splits[\"val\"][1])\n",
    "    big_no_sw_acc = calculate_avg_accuracy(\n",
    "        pred_no_sw_big, amazon_rev_no_sw_splits[\"val\"][1])\n",
    "    uni_big_no_sw_acc = calculate_avg_accuracy(\n",
    "        pred_no_sw_uni_big, amazon_rev_no_sw_splits[\"val\"][1])\n",
    "    \n",
    "    sum_acc_no_sw = uni_no_sw_acc + big_no_sw_acc + uni_big_no_sw_acc\n",
    "    if (sum_acc_no_sw > best_val_seen_so_far_no_sw):\n",
    "        best_val_seen_so_far_no_sw = sum_acc_no_sw\n",
    "        alpha_no_sw = alpha\n",
    "    \n",
    "    # Validation Accuracy With SW\n",
    "    uni_sw_acc = calculate_avg_accuracy(\n",
    "        pred_sw_uni, amazon_rev_sw_splits[\"val\"][1])\n",
    "    big_sw_acc = calculate_avg_accuracy(\n",
    "        pred_sw_big, amazon_rev_sw_splits[\"val\"][1])\n",
    "    uni_big_sw_acc = calculate_avg_accuracy(\n",
    "        pred_sw_uni_big, amazon_rev_sw_splits[\"val\"][1])\n",
    "    \n",
    "    sum_acc_sw = uni_sw_acc + big_sw_acc + uni_big_sw_acc\n",
    "    if (sum_acc_sw > best_val_seen_so_far_sw):\n",
    "        best_val_seen_so_far_sw = sum_acc_sw\n",
    "        alpha_sw = alpha\n",
    "\n",
    "print(f'Best Alpha for Without Stopwords: {alpha_no_sw}')\n",
    "print(f'Best Alpha for With Stopwords: {alpha_sw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Model After Selecting Best Alpha\n"
     ]
    }
   ],
   "source": [
    "print('Retraining Model After Selecting Best Alpha')\n",
    "# Training Without SW\n",
    "unigram_no_sw_classifier = MultinomialNB(alpha=alpha_no_sw).fit(\n",
    "    amazon_rev_no_sw_uni_counts[:len_no_sw_train],\n",
    "    amazon_rev_no_sw_splits['train'][1])\n",
    "bigram_no_sw_classifier = MultinomialNB(alpha=alpha_no_sw).fit(\n",
    "    amazon_rev_no_sw_big_counts[:len_no_sw_train],\n",
    "    amazon_rev_no_sw_splits['train'][1])\n",
    "unigram_bigram_no_sw_classifier = MultinomialNB(alpha=alpha_no_sw).fit(\n",
    "    amazon_rev_no_sw_uni_big_counts[:len_no_sw_train],\n",
    "    amazon_rev_no_sw_splits['train'][1])\n",
    "\n",
    "# Training With SW\n",
    "unigram_sw_classifier = MultinomialNB(alpha=alpha_sw).fit(\n",
    "    amazon_rev_sw_uni_counts[:len_sw_train],\n",
    "    amazon_rev_sw_splits['train'][1])\n",
    "bigram_sw_classifier = MultinomialNB(alpha=alpha_sw).fit(\n",
    "    amazon_rev_sw_big_counts[:len_sw_train],\n",
    "    amazon_rev_sw_splits['train'][1])\n",
    "unigram_bigram_sw_classifier = MultinomialNB(alpha=alpha_sw).fit(\n",
    "    amazon_rev_sw_uni_big_counts[:len_sw_train],\n",
    "    amazon_rev_sw_splits['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classifiers: Without Stopwords\n",
      "Testing Classifiers: With Stopwords\n",
      "Calculating Accuracies\n",
      "--------------------------------------------------\n",
      "With Stopwords:\n",
      "--------------------------------------------------\n",
      "Unigrams: 0.8081375\n",
      "Bigrams: 0.8282875\n",
      "Unigrams+Bigrams: 0.835225\n",
      "--------------------------------------------------\n",
      "Without Stopwords:\n",
      "--------------------------------------------------\n",
      "Unigrams: 0.80405\n",
      "Bigrams: 0.7886625\n",
      "Unigrams+Bigrams: 0.82225\n"
     ]
    }
   ],
   "source": [
    "print('Testing Classifiers: Without Stopwords')\n",
    "pred_no_sw_uni = unigram_no_sw_classifier.predict(\n",
    "    amazon_rev_no_sw_uni_counts[-len_no_sw_val:])\n",
    "pred_no_sw_big = bigram_no_sw_classifier.predict(\n",
    "    amazon_rev_no_sw_big_counts[-len_no_sw_val:])\n",
    "pred_no_sw_uni_big = unigram_bigram_no_sw_classifier.predict(\n",
    "    amazon_rev_no_sw_uni_big_counts[-len_no_sw_val:])\n",
    "\n",
    "print('Testing Classifiers: With Stopwords')\n",
    "pred_sw_uni = unigram_sw_classifier.predict(\n",
    "    amazon_rev_sw_uni_counts[-len_sw_val:])\n",
    "pred_sw_big = bigram_sw_classifier.predict(\n",
    "    amazon_rev_sw_big_counts[-len_sw_val:])\n",
    "pred_sw_uni_big = unigram_bigram_sw_classifier.predict(\n",
    "    amazon_rev_sw_uni_big_counts[-len_sw_val:])\n",
    "\n",
    "print('Calculating Accuracies')\n",
    "print('--------------------------------------------------')\n",
    "print('With Stopwords:')\n",
    "print('--------------------------------------------------')\n",
    "print(f'Unigrams: {calculate_avg_accuracy(pred_sw_uni, amazon_rev_sw_splits[\"test\"][1])}')\n",
    "print(f'Bigrams: {calculate_avg_accuracy(pred_sw_big, amazon_rev_sw_splits[\"test\"][1])}')\n",
    "print(f'Unigrams+Bigrams: {calculate_avg_accuracy(pred_sw_uni_big, amazon_rev_sw_splits[\"test\"][1])}')\n",
    "print('--------------------------------------------------')\n",
    "print('Without Stopwords:')\n",
    "print('--------------------------------------------------')\n",
    "print(f'Unigrams: {calculate_avg_accuracy(pred_no_sw_uni, amazon_rev_no_sw_splits[\"test\"][1])}')\n",
    "print(f'Bigrams: {calculate_avg_accuracy(pred_no_sw_big, amazon_rev_no_sw_splits[\"test\"][1])}')\n",
    "print(f'Unigrams+Bigrams: {calculate_avg_accuracy(pred_no_sw_uni_big, amazon_rev_no_sw_splits[\"test\"][1])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
