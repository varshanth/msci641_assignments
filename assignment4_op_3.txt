[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
----- Dataset Synthesis Start -----
Loading Positive Reviews from dataset/pos.txt
Loading Negative Reviews from dataset/neg.txt
Generating data and labels
Tokenizing the data
Shuffling the data
----- Dataset Synthesis Complete -----
Fitting Tokenizer on Dataset
Splitting Dataset into Train, Val & Test
Creating local embeddings matrix from Word2Vec Embeddings
/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function
  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL
WARNING: Logging before flag parsing goes to stderr.
W0624 21:26:36.298550 140362964252544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Creating Hyperparameter Sequential Search Grid
Executing Hyperparam Sequential Search
----------------------------------------
Hyperparam: activation_fn, Value: sigmoid
Activation: sigmoid
DropoutRate: 0
RegParam: 0
W0624 21:26:39.685929 140362964252544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0624 21:26:39.776034 140362964252544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten (Flatten)            (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00004: early stopping
80000/80000 [==============================] - 4s 47us/sample - loss: 0.4938 - acc: 0.7713
Validation Accuracy: 0.7713375091552734
Hyperparam: activation_fn, Value: tanh
Activation: tanh
DropoutRate: 0
RegParam: 0
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_1 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00006: early stopping
80000/80000 [==============================] - 4s 48us/sample - loss: 0.5513 - acc: 0.7662
Validation Accuracy: 0.7661874890327454
Hyperparam: activation_fn, Value: relu
Activation: relu
DropoutRate: 0
RegParam: 0
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_2 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00007: early stopping
80000/80000 [==============================] - 4s 48us/sample - loss: 0.5856 - acc: 0.7648
Validation Accuracy: 0.7648249864578247
Hyperparam: dropout_rate, Value: 0
Activation: sigmoid
DropoutRate: 0
RegParam: 0
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_3 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00004: early stopping
80000/80000 [==============================] - 4s 49us/sample - loss: 0.4931 - acc: 0.7719
Validation Accuracy: 0.7719249725341797
Hyperparam: dropout_rate, Value: 0.1
Activation: sigmoid
DropoutRate: 0.1
RegParam: 0
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_4 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00004: early stopping
80000/80000 [==============================] - 4s 49us/sample - loss: 0.4987 - acc: 0.7727
Validation Accuracy: 0.7726874947547913
Hyperparam: dropout_rate, Value: 0.3
Activation: sigmoid
DropoutRate: 0.3
RegParam: 0
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_5 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00005: early stopping
80000/80000 [==============================] - 4s 50us/sample - loss: 0.5131 - acc: 0.7716
Validation Accuracy: 0.7715749740600586
Hyperparam: dropout_rate, Value: 0.5
Activation: sigmoid
DropoutRate: 0.5
RegParam: 0
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_6 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00004: early stopping
80000/80000 [==============================] - 4s 50us/sample - loss: 0.4953 - acc: 0.7723
Validation Accuracy: 0.7723249793052673
Hyperparam: reg_param, Value: 0
Activation: sigmoid
DropoutRate: 0.1
RegParam: 0
Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_7 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00005: early stopping
80000/80000 [==============================] - 4s 52us/sample - loss: 0.5141 - acc: 0.7718
Validation Accuracy: 0.7718499898910522
Hyperparam: reg_param, Value: 0.1
Activation: sigmoid
DropoutRate: 0.1
RegParam: 0.1
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_8 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00004: early stopping
80000/80000 [==============================] - 4s 54us/sample - loss: 0.6760 - acc: 0.6893
Validation Accuracy: 0.6892874836921692
Hyperparam: reg_param, Value: 0.01
Activation: sigmoid
DropoutRate: 0.1
RegParam: 0.01
Model: "sequential_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_9 (Flatten)          (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00003: early stopping
80000/80000 [==============================] - 4s 55us/sample - loss: 0.6162 - acc: 0.7175
Validation Accuracy: 0.7174749970436096
Hyperparam: reg_param, Value: 0.001
Activation: sigmoid
DropoutRate: 0.1
RegParam: 0.001
Model: "sequential_10"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
word_embedding_layer (Embedd (None, 26, 300)           24066300
_________________________________________________________________
flatten_10 (Flatten)         (None, 7800)              0
_________________________________________________________________
hidden_layer (Dense)         (None, 1024)              7988224
_________________________________________________________________
dropout_layer (Dropout)      (None, 1024)              0
_________________________________________________________________
output_layer (Dense)         (None, 1)                 1025
=================================================================
Total params: 32,055,549
Trainable params: 7,989,249
Non-trainable params: 24,066,300
_________________________________________________________________
Epoch 00003: early stopping
80000/80000 [==============================] - 5s 56us/sample - loss: 0.5516 - acc: 0.7461
Validation Accuracy: 0.7461000084877014
----------------------------------------
Selected Hyperparameters: {'activation_fn': 'sigmoid', 'dropout_rate': 0.1, 'reg_param': 0}
----------------------------------------
Testing Best Model
80000/80000 [==============================] - 4s 47us/sample - loss: 0.5170 - acc: 0.7745
Test Accuracy: 0.7744874954223633
----------------------------------------
